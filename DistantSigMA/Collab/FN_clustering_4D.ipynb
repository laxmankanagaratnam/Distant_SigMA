{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SigMA on Flame Nebula\n",
    "\n",
    "The idea of this notebook is to run SigMA on the 4D space we have available from VISIONS. Wish me luck!\n",
    "### Modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Python modules\n",
    "import os\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.coordinates import LSR, SkyCoord, Distance\n",
    "import astropy.units as u\n",
    "\n",
    "#### DistantSigMA modules\n",
    "from DistantSigMA.DistantSigMA.cluster_simulations import calculate_std_devs\n",
    "from DistantSigMA.DistantSigMA.scalefactor_sampling import lhc_lloyd\n",
    "from DistantSigMA.misc import utilities as ut\n",
    "from DistantSigMA.DistantSigMA.setup_and_scaling import parameter_scaler, single_parameter_scaling, save_output_summary\n",
    "from SigMA.SigMA import SigMA\n",
    "from DistantSigMA.DistantSigMA.clustering_routine import remove_field_stars, extract_signal_remove_spurious, extract_signal, consensus_function\n",
    "from coordinate_transformations.sky_convert import gal_vtan_lsr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:04.948303Z",
     "start_time": "2025-01-23T11:23:02.962750Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Paths\n",
    "output_path = ut.set_output_path(script_name=\"4D_clustering\")\n",
    "\n",
    "run = \"first_try\"\n",
    "output_path = output_path + f\"{run}/\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:04.951690Z",
     "start_time": "2025-01-23T11:23:04.950145Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "*Note: The input file needs to contain the following columns: ['ra', 'dec', 'parallax', 'pmra', 'pmdec']*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load data for error sampling (already slimmed)\n",
    "error_sampling_df = pd.read_csv(\"../../Data/Gaia/Gaia_DR3_500pc_10percent.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.544644Z",
     "start_time": "2025-01-23T11:23:04.952503Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IR df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "IR_new = ['ra', 'dec', 'parallax', 'pmra', 'pmdec']\n",
    "IR_old = [\"mean_RA\", \"mean_DEC\", \"pmra_IR\", \"pmde_IR\"]\n",
    "ir_dict = dict(zip(IR_old, IR_new))\n",
    "\n",
    "df_IR = pd.read_csv('../../Data/tmp/master-lite_x_Spitzer_Megeath_Gaia_Nemesis.csv', usecols=(\"mean_RA\", \"mean_DEC\", \"pmra_IR\", \"pmde_IR\", \"RAdeg\", \"DEdeg\", \"Plx\", \"pmRA\", \"pmDE\",\n",
    "                                                                                                \"e_RAdeg\", \"e_DEdeg\", \"e_Plx\", \"e_pmRA\", \"e_pmDE\"))\n",
    "\n",
    "df_IR.rename(columns=ir_dict, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.604086Z",
     "start_time": "2025-01-23T11:23:05.546219Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gaia df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "gaia_old = [\"RAdeg\", \"DEdeg\", \"Plx\", \"pmRA\", \"pmDE\", \"e_RAdeg\", \"e_DEdeg\", \"e_Plx\", \"e_pmRA\", \"e_pmDE\"]\n",
    "gaia_new = ['ra', 'dec', 'parallax', 'pmra', 'pmdec', \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra_error\", \"pmde_error\"]\n",
    "gaia_dict = dict(zip(gaia_old, gaia_new))\n",
    "\n",
    "# load the dataframe - need 2 for this because of the naming conventions\n",
    "df_gaia = pd.read_csv('../../Data/tmp/master-lite_x_Spitzer_Megeath_Gaia_Nemesis.csv', usecols=(\"RAdeg\", \"DEdeg\", \"Plx\", \"pmRA\", \"pmDE\",\n",
    "                                                                                                \"e_RAdeg\", \"e_DEdeg\", \"e_Plx\", \"e_pmRA\", \"e_pmDE\"))\n",
    "\n",
    "df_gaia.rename(columns=gaia_dict, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.662269Z",
     "start_time": "2025-01-23T11:23:05.607980Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add cartesian coordinates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: negative parallaxes are converted to NaN distances even when `allow_negative=True`, because negative parallaxes cannot be transformed into distances. See the discussion in this paper: https://arxiv.org/abs/1507.02105 [astropy.coordinates.distances]\n"
     ]
    }
   ],
   "source": [
    "skycoord = SkyCoord(\n",
    "        ra=df_gaia[\"ra\"].to_numpy() * u.deg,\n",
    "        dec=df_gaia[\"dec\"].to_numpy() * u.deg,  # 2D on sky postition\n",
    "        distance=Distance(parallax=df_gaia[\"parallax\"].to_numpy() * u.mas, allow_negative=True),  # distance in pc\n",
    "        pm_ra_cosdec=df_gaia[\"pmra\"].to_numpy() * u.mas / u.yr,\n",
    "        pm_dec=df_gaia[\"pmdec\"].to_numpy() * u.mas / u.yr,\n",
    "        radial_velocity=0.0 * u.km / u.s,\n",
    "        frame=\"icrs\",\n",
    "    )\n",
    "\n",
    "df = gal_vtan_lsr(skycoord)\n",
    "df_chunk = pd.concat([df_gaia, df], axis=1)\n",
    "df_chunk.dropna(subset=[\"ra\", \"dec\", \"parallax\", \"pmra\", \"pmdec\"], inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.694788Z",
     "start_time": "2025-01-23T11:23:05.668895Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### constrain the Distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_chunk[\"D\"] = 1000/df_chunk[\"parallax\"]\n",
    "\n",
    "df_small = df_chunk[(df_chunk.D > 10) & (df_chunk.D < 1000)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.697848Z",
     "start_time": "2025-01-23T11:23:05.696098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# set variables manually as there is no coarse segmentation in the input dataset right now\n",
    "chunk = 0\n",
    "result_path = output_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.700674Z",
     "start_time": "2025-01-23T11:23:05.698939Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "          ra       dec  ra_error  dec_error  parallax  parallax_error   pmra  \\\n0  85.551753 -2.086052    0.0237     0.0214    2.5119          0.0274  1.424   \n1  85.386485 -1.965999    0.0220     0.0207    2.6756          0.0285  2.093   \n3  85.260554 -1.727195    1.1237     1.0010    3.6669          1.4756 -1.335   \n5  85.321513 -1.731072    0.2782     0.2881    2.4825          0.4087  0.377   \n7  85.326645 -1.831449    0.1863     0.1996    2.7434          0.3040 -1.510   \n\n   pmra_error  pmdec  pmde_error           X           Y           Z  \\\n0       0.028  0.330       0.024 -341.124422 -172.016934 -111.946015   \n1       0.026 -1.232       0.024 -320.641056 -160.350540 -105.662402   \n3       1.221  0.369       1.042 -234.531613 -115.851212  -77.098112   \n5       0.322 -0.389       0.322 -346.419964 -171.370401 -113.530330   \n7       0.257 -0.343       0.231 -313.149436 -155.558950 -102.991515   \n\n    v_a_lsr   v_d_lsr           D  \n0  2.251009  7.092503  398.105020  \n1  3.224025  4.322255  373.747944  \n3 -2.246544  7.052169  272.709918  \n5  0.216861  5.831167  402.819738  \n7 -3.110767  5.951858  364.511190  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ra</th>\n      <th>dec</th>\n      <th>ra_error</th>\n      <th>dec_error</th>\n      <th>parallax</th>\n      <th>parallax_error</th>\n      <th>pmra</th>\n      <th>pmra_error</th>\n      <th>pmdec</th>\n      <th>pmde_error</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Z</th>\n      <th>v_a_lsr</th>\n      <th>v_d_lsr</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>85.551753</td>\n      <td>-2.086052</td>\n      <td>0.0237</td>\n      <td>0.0214</td>\n      <td>2.5119</td>\n      <td>0.0274</td>\n      <td>1.424</td>\n      <td>0.028</td>\n      <td>0.330</td>\n      <td>0.024</td>\n      <td>-341.124422</td>\n      <td>-172.016934</td>\n      <td>-111.946015</td>\n      <td>2.251009</td>\n      <td>7.092503</td>\n      <td>398.105020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85.386485</td>\n      <td>-1.965999</td>\n      <td>0.0220</td>\n      <td>0.0207</td>\n      <td>2.6756</td>\n      <td>0.0285</td>\n      <td>2.093</td>\n      <td>0.026</td>\n      <td>-1.232</td>\n      <td>0.024</td>\n      <td>-320.641056</td>\n      <td>-160.350540</td>\n      <td>-105.662402</td>\n      <td>3.224025</td>\n      <td>4.322255</td>\n      <td>373.747944</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>85.260554</td>\n      <td>-1.727195</td>\n      <td>1.1237</td>\n      <td>1.0010</td>\n      <td>3.6669</td>\n      <td>1.4756</td>\n      <td>-1.335</td>\n      <td>1.221</td>\n      <td>0.369</td>\n      <td>1.042</td>\n      <td>-234.531613</td>\n      <td>-115.851212</td>\n      <td>-77.098112</td>\n      <td>-2.246544</td>\n      <td>7.052169</td>\n      <td>272.709918</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>85.321513</td>\n      <td>-1.731072</td>\n      <td>0.2782</td>\n      <td>0.2881</td>\n      <td>2.4825</td>\n      <td>0.4087</td>\n      <td>0.377</td>\n      <td>0.322</td>\n      <td>-0.389</td>\n      <td>0.322</td>\n      <td>-346.419964</td>\n      <td>-171.370401</td>\n      <td>-113.530330</td>\n      <td>0.216861</td>\n      <td>5.831167</td>\n      <td>402.819738</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>85.326645</td>\n      <td>-1.831449</td>\n      <td>0.1863</td>\n      <td>0.1996</td>\n      <td>2.7434</td>\n      <td>0.3040</td>\n      <td>-1.510</td>\n      <td>0.257</td>\n      <td>-0.343</td>\n      <td>0.231</td>\n      <td>-313.149436</td>\n      <td>-155.558950</td>\n      <td>-102.991515</td>\n      <td>-3.110767</td>\n      <td>5.951858</td>\n      <td>364.511190</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.725083Z",
     "start_time": "2025-01-23T11:23:05.700889Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering\n",
    "\n",
    "Currently written for the scenario that the input is a single chunk without need for further splitting.\n",
    "\n",
    "### A) Preliminary solution -- Gaia data\n",
    "\n",
    "My idea is to make the preliminary cluster using the subset of the sources with Gaia data -- this way we should be able to determine the scaling factors nicely. *Maybe I will need to add more Gaia sources in the region if it does not work at first.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def setup_4D(df_fit: pd.DataFrame, sf_range: Union[list, np.linspace, range],\n",
    "                  KNN_list: Union[list, np.linspace, range], beta: float, knn_initcluster_graph: int,\n",
    "                  scaling: str = None, means=None):\n",
    "\n",
    "    # resampling is not implemented for ICRS yet\n",
    "    n_resampling = 0\n",
    "\n",
    "    # scale the ICRS parameters\n",
    "    scaled_cols = ['ra', 'dec', 'pmra', 'pmdec']\n",
    "    df_fin = df_fit\n",
    "    scale_factors = means\n",
    "\n",
    "    # SigMA kwargs\n",
    "    sigma_kwargs = dict(cluster_features=scaled_cols, scale_factors=scale_factors, nb_resampling=n_resampling,\n",
    "                        max_knn_density=max(KNN_list) + 1, beta=beta, knn_initcluster_graph=knn_initcluster_graph)\n",
    "\n",
    "    setup_dict = {\"scale_factor_list\": sf_range, \"scale_factors\": scale_factors, \"sigma_kwargs\": sigma_kwargs}\n",
    "\n",
    "    return setup_dict, df_fin"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.740975Z",
     "start_time": "2025-01-23T11:23:05.725253Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def setup_ICRS_ps(df_fit: pd.DataFrame, sf_params: Union[str, list], sf_range: Union[list, np.linspace, range],\n",
    "                  KNN_list: Union[list, np.linspace, range], beta: float, knn_initcluster_graph: int,\n",
    "                  scaling: str = None, means=None, cols_to_scale = None):\n",
    "\n",
    "    n_resampling = 0\n",
    "\n",
    "    # scale the ICRS parameters\n",
    "    if type(scaling) == str:\n",
    "        df_scaled = df_fit.copy()\n",
    "        if cols_to_scale is None:\n",
    "            cols_to_scale = ['ra', 'dec', 'parallax', 'pmra', 'pmdec']\n",
    "        else:\n",
    "            cols_to_scale = cols_to_scale\n",
    "        scaled_cols = ['ra_scaled', 'dec_scaled', 'parallax_scaled', 'pmra_scaled', 'pmdec_scaled']\n",
    "        scaled_data = [parameter_scaler(df_scaled[col], scaling) for col in cols_to_scale]\n",
    "\n",
    "        for col_id, col in enumerate(scaled_cols):\n",
    "            df_scaled[col] = scaled_data[col_id]\n",
    "\n",
    "        df_fin = df_scaled\n",
    "\n",
    "    else:\n",
    "        scaled_cols = cols_to_scale\n",
    "        df_fin = df_fit\n",
    "\n",
    "    if type(sf_params) == str:\n",
    "        mean_sf, scale_factors = single_parameter_scaling(sf_params, sf_range)\n",
    "    elif type(sf_params) == list:\n",
    "        scale_factors = means\n",
    "\n",
    "    # SigMA kwargs\n",
    "    sigma_kwargs = dict(cluster_features=scaled_cols, scale_factors=scale_factors, nb_resampling=n_resampling,\n",
    "                        max_knn_density=max(KNN_list) + 1, beta=beta, knn_initcluster_graph=knn_initcluster_graph)\n",
    "\n",
    "    setup_dict = {\"scale_factor_list\": sf_range, \"scale_factors\": scale_factors, \"sigma_kwargs\": sigma_kwargs}\n",
    "\n",
    "    return setup_dict, df_fin\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.749063Z",
     "start_time": "2025-01-23T11:23:05.725427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def run_clustering(region_label, df_input, sf_params, parameter_dict: dict, mode: str, output_loc: str, setup_func,\n",
    "                   column_means=None, cols_to_scale =None ):\n",
    "    # most important variables\n",
    "    KNNs = parameter_dict[\"KNN_list\"]\n",
    "    # setup kwargs\n",
    "    if setup_func == \"ICRS\":\n",
    "        setup_kwargs, df_focus = setup_ICRS_ps(df_fit=df_input, sf_params=sf_params, sf_range=parameter_dict[\"sfs\"],\n",
    "                                               KNN_list=KNNs, beta=parameter_dict[\"beta\"],\n",
    "                                               knn_initcluster_graph=parameter_dict[\"knn_initcluster_graph\"],\n",
    "                                               scaling=parameter_dict[\"scaling\"], means=column_means, cols_to_scale=cols_to_scale)\n",
    "    else:\n",
    "        setup_kwargs, df_focus = setup_4D(df_fit = df_input, sf_range=parameter_dict[\"sfs\"],  KNN_list=KNNs, beta=parameter_dict[\"beta\"],\n",
    "                                               knn_initcluster_graph=parameter_dict[\"knn_initcluster_graph\"],\n",
    "                                               scaling=parameter_dict[\"scaling\"], means=column_means, )\n",
    "    sigma_kwargs = setup_kwargs[\"sigma_kwargs\"]\n",
    "    scale_factor_list = setup_kwargs[\"scale_factor_list\"]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # initialize SigMA with sf_mean\n",
    "    clusterer = SigMA(data=df_focus, **sigma_kwargs)\n",
    "    # save X_mean\n",
    "    X_mean_sf = clusterer.X\n",
    "    # initialize array for density values (collect the rho_sums)\n",
    "    rhosum_list = []\n",
    "\n",
    "    # Initialize array for the outer cc (occ) results (remove field stars / rfs, remove spurious clusters / rsc)\n",
    "    results_rfs = np.empty(shape=(len(KNNs), len(df_focus)))\n",
    "    results_rsc = np.empty(shape=(len(KNNs), len(df_focus)))\n",
    "    results_simple = np.empty(shape=(len(KNNs), len(df_focus)))\n",
    "\n",
    "    # Outer loop: KNN\n",
    "    for kid, knn in enumerate(KNNs):\n",
    "\n",
    "        print(f\"-- Current run with KNN = {knn} -- \\n\")\n",
    "\n",
    "        label_matrix_rfs = np.empty(shape=(len(scale_factor_list), len(df_focus)))\n",
    "        label_matrix_rsc = np.empty(shape=(len(scale_factor_list), len(df_focus)))\n",
    "        label_matrix_simple = np.empty(shape=(len(scale_factor_list), len(df_focus)))\n",
    "\n",
    "        # initialize density-sum over all scaling factors\n",
    "        rho_sum = np.zeros(df_focus.shape[0], dtype=np.float32)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        df_labels = pd.DataFrame()\n",
    "        # Inner loop: Scale factors\n",
    "        for sf_id, sf in enumerate(scale_factor_list):\n",
    "            # Set current scale factor\n",
    "            if mode == \"prelim\":\n",
    "                scale_factors = {'pos': {'features': ['parallax_scaled'], 'factor': sf}}\n",
    "                clusterer.set_scaling_factors(scale_factors)\n",
    "                print(f\"Performing clustering for scale factor {clusterer.scale_factors['pos']['factor']}...\")\n",
    "            elif mode == \"final\":\n",
    "                scale_factors = {'pos': {'features': ['ra', 'dec'], 'factor': list(sf[:3])},\n",
    "                                 'vel': {'features': ['pmra', 'pmdec'], 'factor': list(sf[3:])}}\n",
    "                clusterer.set_scaling_factors(scale_factors)\n",
    "                print(f\"Performing clustering for scale factor p{clusterer.scale_factors['pos']['factor']}\"\n",
    "                      f\"{clusterer.scale_factors['vel']['factor']}...\")\n",
    "\n",
    "            # Fit\n",
    "            clusterer.fit(alpha=parameter_dict[\"alpha\"], knn=knn, bh_correction=parameter_dict[\"bh_correction\"])\n",
    "            label_array = clusterer.labels_\n",
    "\n",
    "            # density and X\n",
    "            rho, X = clusterer.weights_, clusterer.X\n",
    "            rho_sum += rho\n",
    "\n",
    "            # a) remove field stars\n",
    "            nb_rfs = remove_field_stars(label_array, rho, label_matrix_rfs, sf_id)\n",
    "            # b) remove spurious clusters\n",
    "            nb_es, nb_rsc = extract_signal_remove_spurious(df_focus, label_array, rho, X, label_matrix_rsc, sf_id)\n",
    "            # c) do new method\n",
    "            nb_simple = extract_signal(label_array, clusterer, label_matrix_simple, sf_id)\n",
    "            # Write the output to the hyperparameter file:\n",
    "\n",
    "            save_output_summary(\n",
    "                summary_str={\"knn\": knn, \"sf\": str(sf), \"n_rfs\": nb_rfs, \"n_rsc\": nb_rsc, \"n_simple\": nb_simple},\n",
    "                file=output_loc + f\"{mode}_ICC_{knn}_summary.csv\")\n",
    "\n",
    "            df_labels[f\"#_{sf_id}\"] = label_matrix_rsc[sf_id, :]\n",
    "\n",
    "        if mode == \"final\":\n",
    "            # save output files\n",
    "            df_labels.to_csv(output_loc + f\"{mode}_ICC_{knn}_labels.csv\")\n",
    "        # append the density sum to the list over all KNN\n",
    "        rhosum_list.append(rho_sum)\n",
    "\n",
    "        # Perform consensus clustering on the a) and b) arrays (automatically generates and saves a html-plot)\n",
    "        labels_icc_rfs, n_icc_rfs = consensus_function(label_matrix_rfs, rho_sum, df_focus,\n",
    "                                                       f\"{mode}_Region_{int(region_label)}_rfs_KNN_{knn}_ICC\",\n",
    "                                                       output_loc,\n",
    "                                                       plotting=True)\n",
    "        labels_icc_rsc, n_icc_rsc = consensus_function(label_matrix_rsc, rho_sum, df_focus,\n",
    "                                                       f\"{mode}_Region_{int(region_label)}_rsc_KNN_{knn}_ICC\",\n",
    "                                                       output_loc,\n",
    "                                                       plotting=True)\n",
    "\n",
    "        labels_icc_simple, n_icc_simple = consensus_function(label_matrix_simple, rho_sum, df_focus,\n",
    "                                                             f\"{mode}_Region_{int(region_label)}_simple_KNN_{knn}_ICC\",\n",
    "                                                             output_loc,\n",
    "                                                             plotting=True)\n",
    "        results_rfs[kid, :] = labels_icc_rfs\n",
    "\n",
    "        results_rsc[kid, :] = labels_icc_rsc\n",
    "        results_simple[kid, :] = labels_icc_simple\n",
    "\n",
    "        print(f\":: Finished run for KNN={knn}! \\n. Found {n_icc_rfs} / {n_icc_rsc} / {n_icc_simple} final clusters.\")\n",
    "\n",
    "    knn_mid = int(len(KNNs) / 2 - 1)\n",
    "    df_save = df_focus.copy()\n",
    "    label_lists = [results_rfs, results_rsc, results_simple]\n",
    "\n",
    "    # Perform consensus clustering on the c) and d) steps\n",
    "    labels_occ, n_occ = zip(\n",
    "        *(consensus_function(jl, rhosum_list[knn_mid], df_focus, f\"{mode}_Region_{int(region_label)}_{name}_OCC\",\n",
    "                             output_loc) for jl, name in zip(label_lists, [\"rfs\", \"rsc\", \"simple\"])))\n",
    "    n_occ = list(n_occ)\n",
    "    labels_occ = list(labels_occ)\n",
    "\n",
    "    # save the labels in a csv file and plot the result\n",
    "    df_save[\"rsc\"] = labels_occ[1]\n",
    "    df_save[\"rfs\"] = labels_occ[0]\n",
    "    df_save[\"simple\"] = labels_occ[2]\n",
    "    df_save.to_csv(output_loc + f\"{mode}_Region_{int(region_label)}_results_CC.csv\")\n",
    "\n",
    "    save_output_summary(summary_str={\"knn\": \"occ\", \"n_rfs\": n_occ[0], \"n_rsc\": n_occ[1],\n",
    "                                     \"n_rfs_cleaned\": n_occ[2]},\n",
    "                        file=output_loc + f\"{mode}_Region_{int(region_label)}_outer_summary.csv\")\n",
    "\n",
    "    # Output log-file\n",
    "    filename = output_loc + f\"Region_{region_label}_{mode}_parameters.txt\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for key, value in parameter_dict.items():\n",
    "            file.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    return df_save\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.749483Z",
     "start_time": "2025-01-23T11:23:05.725500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Cluster parameters\n",
    "dict_prelim = dict(alpha=0.01,\n",
    "                   beta=0.99,\n",
    "                   knn_initcluster_graph=35, # slightly larger than the bigget KNN\n",
    "                   KNN_list=[15,20, 25],\n",
    "                   sfs=[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55],\n",
    "                   scaling=\"robust\",\n",
    "                   bh_correction=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:05.767194Z",
     "start_time": "2025-01-23T11:23:05.736251Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART A) Starting clustering ... \n",
      "\n",
      "-- Current run with KNN = 15 -- \n",
      "\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.1...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.15...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.2...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.25...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.3...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.35...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.4...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.45...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.5...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.55...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      ":: Finished run for KNN=15! \n",
      ". Found 1 / 1 / 1 final clusters.\n",
      "-- Current run with KNN = 20 -- \n",
      "\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.1...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.15...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.2...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.25...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.3...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.35...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.4...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.45...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.5...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.55...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      ":: Finished run for KNN=20! \n",
      ". Found 1 / 1 / 1 final clusters.\n",
      "-- Current run with KNN = 25 -- \n",
      "\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.1...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.15...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.2...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.25...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.3...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.35...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.4...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.45...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.5...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.55...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      ":: Finished run for KNN=25! \n",
      ". Found 1 / 1 / 1 final clusters.\n"
     ]
    }
   ],
   "source": [
    "print(f\"PART A) Starting clustering ... \\n\")\n",
    "\n",
    "df_prelim = run_clustering(region_label=chunk, df_input=df_small, sf_params=\"parallax_scaled\",\n",
    "                           parameter_dict=dict_prelim, mode=\"prelim\", output_loc=result_path, setup_func=\"ICRS\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:10.800145Z",
     "start_time": "2025-01-23T11:23:05.741126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n",
      "507\n"
     ]
    },
    {
     "data": {
      "text/plain": "381.38825324180016"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d =df_prelim.dropna()\n",
    "d = d[d.parallax >= 0]\n",
    "\n",
    "print(len(df_prelim))\n",
    "print(len(d))\n",
    "\n",
    "d.rename(columns={\"Plx\":'parallax'}, inplace=True)\n",
    "d[\"radial_velocity\"] = pd.NA\n",
    "d[\"D\"] = 1000/d[\"parallax\"]\n",
    "\n",
    "c1 = d[d[\"rsc\"] == 0]\n",
    "c1.D.median()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:10.807502Z",
     "start_time": "2025-01-23T11:23:10.801456Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B) Simulate clusters and determine scale factors\n",
    "\n",
    "We now want to use the preliminary solution (mostly cluster cores) to determine more appropriate scaling factors for the clustering algorithm. We calculate them and save them in a text file. It is also possible to add a couple of small, artificial clusters to make the algorithm more sensitive for small clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART B) Simulating clusters ... \n",
      "\n",
      "382.57971433751044\n",
      "382.57971433751044\n",
      "382.57971433751044\n",
      "404.58926864122793\n",
      "404.58926864122793\n",
      "404.58926864122793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Sigma_Orion/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"PART B) Simulating clusters ... \\n\")\n",
    "\n",
    "stds = calculate_std_devs(input_df=d, SigMA_dict=dict_prelim, sampling_data= error_sampling_df, n_artificial = 4, sample_radius= 2, output_path = result_path, plot_figs = False)\n",
    "\n",
    "# save scaling factors in Data directory\n",
    "directory = \"../../Data/Scale_factors\"\n",
    "filename = f\"sfs_FN_{chunk}.txt\"\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open(f\"{directory}/{filename}\", 'w') as file:\n",
    "    for i, label in enumerate([\"ra\", \"dec\", \"parallax\", \"pmra\", \"pmdec\"]):\n",
    "        print(f\"{label}:\", np.min(stds[i, :]), np.mean(stds[i, :]), np.max(stds[i, :]), file=file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:23:10.948864Z",
     "start_time": "2025-01-23T11:23:10.809964Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### C) Cluster with new SF\n",
    "\n",
    "We apply the new scaling factors to the clustering algorithm. You can play with the number of scaling factors - more factors = more computing time, but hopefully also a more reliable result, as the parameter space is sampled more densely."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alena/PycharmProjects/Distant_SigMA/DistantSigMA/DistantSigMA/scalefactor_sampling.py:13: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine the number of SF to draw using lhc_lloyd sampling\n",
    "num_sf = 30\n",
    "# draw number of scale factors\n",
    "sfs, means = lhc_lloyd('../../Data/Scale_factors/' + f\"sfs_FN_{chunk}.txt\", num_sf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:28:44.160267Z",
     "start_time": "2025-01-23T11:28:44.151683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf],\n       [inf, inf, inf, inf, inf]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:28:57.187640Z",
     "start_time": "2025-01-23T11:28:57.183552Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alena/PycharmProjects/Distant_SigMA/DistantSigMA/DistantSigMA/scalefactor_sampling.py:13: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['pmdec'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 18\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# dict for final clustering\u001B[39;00m\n\u001B[1;32m      9\u001B[0m dict_final \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m,\n\u001B[1;32m     10\u001B[0m                   beta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.99\u001B[39m,\n\u001B[1;32m     11\u001B[0m                   knn_initcluster_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m35\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m                   scaling\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     15\u001B[0m                   bh_correction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 18\u001B[0m df_fin \u001B[38;5;241m=\u001B[39m \u001B[43mrun_clustering\u001B[49m\u001B[43m(\u001B[49m\u001B[43mregion_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf_IR\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m                                \u001B[49m\u001B[43msf_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mra\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdec\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpmra\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpmdec\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mparameter_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdict_final\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfinal\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_loc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresult_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mcolumn_means\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscale_factor_means\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msetup_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m4D\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 21\u001B[0m, in \u001B[0;36mrun_clustering\u001B[0;34m(region_label, df_input, sf_params, parameter_dict, mode, output_loc, setup_func, column_means, cols_to_scale)\u001B[0m\n\u001B[1;32m     16\u001B[0m scale_factor_list \u001B[38;5;241m=\u001B[39m setup_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale_factor_list\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# ---------------------------------------------------------\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# initialize SigMA with sf_mean\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m clusterer \u001B[38;5;241m=\u001B[39m \u001B[43mSigMA\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf_focus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msigma_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# save X_mean\u001B[39;00m\n\u001B[1;32m     23\u001B[0m X_mean_sf \u001B[38;5;241m=\u001B[39m clusterer\u001B[38;5;241m.\u001B[39mX\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/SigMA/SigMA.py:47\u001B[0m, in \u001B[0;36mSigMA.__init__\u001B[0;34m(self, data, cluster_features, scale_factors, nb_resampling, max_knn_density, beta, knn_initcluster_graph, do_remove_edges, hypothesis_test, transform_function, kd_tree_data)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, (pd\u001B[38;5;241m.\u001B[39mDataFrame, pd\u001B[38;5;241m.\u001B[39mSeries)):\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData input needs to be pandas DataFrame!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 47\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# DataLayer class attributes\u001B[39;49;00m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcluster_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscale_factors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscale_factors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# DensityEstimator\u001B[39;49;00m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_knn_density\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_knn_density\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# GraphSkeleton\u001B[39;49;00m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mknn_initcluster_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mknn_initcluster_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdo_remove_edges\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_remove_edges\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Resampling\u001B[39;49;00m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransform_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# KDTree\u001B[39;49;00m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkd_tree_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkd_tree_data\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhypothesis_test \u001B[38;5;241m=\u001B[39m hypothesis_test\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Resampling info\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/SigMA/Parameters.py:113\u001B[0m, in \u001B[0;36mParameterClass.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    110\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    :param max_neighbors: Maximal number of k to consider for density estimation\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Density estimation parameter\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdists_to_knn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# k-distance; used in p-value computation\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/SigMA/GraphSkeleton.py:22\u001B[0m, in \u001B[0;36mGraphSkeleton.__init__\u001B[0;34m(self, knn_initcluster_graph, beta, do_remove_edges, **kwargs)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, knn_initcluster_graph: \u001B[38;5;28mint\u001B[39m, beta: \u001B[38;5;28mfloat\u001B[39m, do_remove_edges: \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     17\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m    :param max_neighbors: maximal number of neighbors considered as graph edges\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m    :param beta: structure parameter of beta skeleton\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;124;03m    :param knn: Number of neighbors for\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_remove_edges \u001B[38;5;241m=\u001B[39m do_remove_edges\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mknn_initcluster_graph \u001B[38;5;241m=\u001B[39m knn_initcluster_graph\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/SigMA/DensityEstimator.py:11\u001B[0m, in \u001B[0;36mDensityEstimator.__init__\u001B[0;34m(self, max_knn_density, **kwargs)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, max_knn_density: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m      8\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Class calculating densities on given data X\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m    max_knn_density: maximal neighbors to consider in knn density estimation\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_knn_density \u001B[38;5;241m=\u001B[39m max_knn_density\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistances \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalc_distances()\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/SigMA/Resampling.py:14\u001B[0m, in \u001B[0;36mPerturbedData.__init__\u001B[0;34m(self, transform_function, **kwargs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, transform_function\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     10\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Creating perturbed data sampled from normal density centered on data points\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m    data: data containing ra, dec, parallax, pmra, pmdec\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m          and corresponding error features + correlations of errors for covariance matrix\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform_function \u001B[38;5;241m=\u001B[39m transform_function\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# Define necessary features\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/SigMA/DataLayer.py:28\u001B[0m, in \u001B[0;36mDataLayer.__init__\u001B[0;34m(self, data, cluster_features, scale_factors, kd_tree_data, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_factors \u001B[38;5;241m=\u001B[39m scale_factors\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkd_tree_data \u001B[38;5;241m=\u001B[39m kd_tree_data\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_cluster_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkd_tree \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_kd_tree(kd_tree_data)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Meta data\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/SigMA/DataLayer.py:55\u001B[0m, in \u001B[0;36mDataLayer.init_cluster_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_cluster_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 55\u001B[0m     X \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcluster_columns\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_factors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m scale_info \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_factors\u001B[38;5;241m.\u001B[39mvalues():\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Sigma_Orion/lib/python3.9/site-packages/pandas/core/frame.py:3811\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3809\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[1;32m   3810\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[0;32m-> 3811\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[1;32m   3814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Sigma_Orion/lib/python3.9/site-packages/pandas/core/indexes/base.py:6113\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[0;34m(self, key, axis_name)\u001B[0m\n\u001B[1;32m   6110\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   6111\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[0;32m-> 6113\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6115\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[1;32m   6116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[1;32m   6117\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Sigma_Orion/lib/python3.9/site-packages/pandas/core/indexes/base.py:6176\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[0;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[1;32m   6173\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6175\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m-> 6176\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['pmdec'] not in index\""
     ]
    }
   ],
   "source": [
    "# draw number of scale factors\n",
    "sfs, means = lhc_lloyd('../../Data/Scale_factors/' + f\"sfs_FN_{chunk}.txt\", num_sf)\n",
    "\n",
    "# determine means for clusterer initialization\n",
    "scale_factor_means = {'pos': {'features': ['ra', 'dec'], 'factor': list(means[:2])},\n",
    "                      'vel': {'features': ['pmra', 'pmdec'], 'factor': list(means[3:])}}\n",
    "\n",
    "# dict for final clustering\n",
    "dict_final = dict(alpha=0.01,\n",
    "                  beta=0.99,\n",
    "                  knn_initcluster_graph=35,\n",
    "                  KNN_list=[15, 20, 25],\n",
    "                  sfs=sfs,\n",
    "                  scaling=None,\n",
    "                  bh_correction=False)\n",
    "\n",
    "\n",
    "df_fin = run_clustering(region_label=chunk, df_input=df_IR,\n",
    "                                sf_params=[\"ra\", \"dec\", \"pmra\", \"pmdec\"],\n",
    "                                parameter_dict=dict_final, mode=\"final\", output_loc=result_path,\n",
    "                                column_means=scale_factor_means, setup_func=\"4D\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T11:26:47.822125Z",
     "start_time": "2025-01-23T11:26:47.734903Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output\n",
    "\n",
    "The code produces a lot of output plots (html). They can be split into the overall results (OCC), which show three different noise removal methods:\n",
    "- rfs (less strict, sometimes spurious clusters)\n",
    "- rsc (more strict, default solution for me)\n",
    "- simple (between rfs and rsc)\n",
    "\n",
    "It also produces plots for the individual solutions for the different KNN."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
