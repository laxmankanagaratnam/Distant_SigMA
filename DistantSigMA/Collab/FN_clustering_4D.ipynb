{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SigMA on Flame Nebula\n",
    "\n",
    "The idea of this notebook is to run SigMA on the 4D space we have available from VISIONS. Wish me luck!\n",
    "### Modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Python modules\n",
    "import os\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.coordinates import LSR, SkyCoord, Distance\n",
    "import astropy.units as u\n",
    "\n",
    "#### DistantSigMA modules\n",
    "from DistantSigMA.DistantSigMA.cluster_simulations import calculate_std_devs\n",
    "from DistantSigMA.DistantSigMA.scalefactor_sampling import lhc_lloyd\n",
    "from DistantSigMA.misc import utilities as ut\n",
    "from DistantSigMA.DistantSigMA.setup_and_scaling import parameter_scaler, single_parameter_scaling, save_output_summary\n",
    "from SigMA.SigMA import SigMA\n",
    "from DistantSigMA.DistantSigMA.clustering_routine import remove_field_stars, extract_signal_remove_spurious, extract_signal, consensus_function\n",
    "from coordinate_transformations.sky_convert import gal_vtan_lsr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:32.835342Z",
     "start_time": "2025-01-21T17:12:31.003492Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Paths\n",
    "output_path = ut.set_output_path(script_name=\"4D_clustering\")\n",
    "\n",
    "run = \"first_try\"\n",
    "output_path = output_path + f\"{run}/\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:32.838437Z",
     "start_time": "2025-01-21T17:12:32.836436Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "*Note: The input file needs to contain the following columns: ['ra', 'dec', 'parallax', 'pmra', 'pmdec']*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load data for error sampling (already slimmed)\n",
    "error_sampling_df = pd.read_csv(\"../../Data/Gaia/Gaia_DR3_500pc_10percent.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.423296Z",
     "start_time": "2025-01-21T17:12:32.838990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IR df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "IR_new = ['ra', 'dec', 'parallax', 'pmra', 'pmdec']\n",
    "IR_old = [\"mean_RA\", \"mean_DEC\", \"pmra_IR\", \"pmde_IR\"]\n",
    "ir_dict = dict(zip(IR_old, IR_new))\n",
    "\n",
    "df_IR = pd.read_csv('../../Data/tmp/master-lite_x_Spitzer_Megeath_Gaia_Nemesis.csv', usecols=(\"mean_RA\", \"mean_DEC\", \"pmra_IR\", \"pmde_IR\", \"RAdeg\", \"DEdeg\", \"Plx\", \"pmRA\", \"pmDE\",\n",
    "                                                                                                \"e_RAdeg\", \"e_DEdeg\", \"e_Plx\", \"e_pmRA\", \"e_pmDE\"))\n",
    "\n",
    "df_IR.rename(columns=ir_dict, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.483340Z",
     "start_time": "2025-01-21T17:12:33.424811Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gaia df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "gaia_old = [\"RAdeg\", \"DEdeg\", \"Plx\", \"pmRA\", \"pmDE\", \"e_RAdeg\", \"e_DEdeg\", \"e_Plx\", \"e_pmRA\", \"e_pmDE\"]\n",
    "gaia_new = ['ra', 'dec', 'parallax', 'pmra', 'pmdec', \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra_error\", \"pmde_error\"]\n",
    "gaia_dict = dict(zip(gaia_old, gaia_new))\n",
    "\n",
    "# load the dataframe - need 2 for this because of the naming conventions\n",
    "df_gaia = pd.read_csv('../../Data/tmp/master-lite_x_Spitzer_Megeath_Gaia_Nemesis.csv', usecols=(\"RAdeg\", \"DEdeg\", \"Plx\", \"pmRA\", \"pmDE\",\n",
    "                                                                                                \"e_RAdeg\", \"e_DEdeg\", \"e_Plx\", \"e_pmRA\", \"e_pmDE\"))\n",
    "\n",
    "df_gaia.rename(columns=gaia_dict, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.541991Z",
     "start_time": "2025-01-21T17:12:33.486103Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add cartesian coordinates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: negative parallaxes are converted to NaN distances even when `allow_negative=True`, because negative parallaxes cannot be transformed into distances. See the discussion in this paper: https://arxiv.org/abs/1507.02105 [astropy.coordinates.distances]\n"
     ]
    }
   ],
   "source": [
    "skycoord = SkyCoord(\n",
    "        ra=df_gaia[\"ra\"].to_numpy() * u.deg,\n",
    "        dec=df_gaia[\"dec\"].to_numpy() * u.deg,  # 2D on sky postition\n",
    "        distance=Distance(parallax=df_gaia[\"parallax\"].to_numpy() * u.mas, allow_negative=True),  # distance in pc\n",
    "        pm_ra_cosdec=df_gaia[\"pmra\"].to_numpy() * u.mas / u.yr,\n",
    "        pm_dec=df_gaia[\"pmdec\"].to_numpy() * u.mas / u.yr,\n",
    "        radial_velocity=0.0 * u.km / u.s,\n",
    "        frame=\"icrs\",\n",
    "    )\n",
    "\n",
    "df = gal_vtan_lsr(skycoord)\n",
    "df_chunk = pd.concat([df_gaia, df], axis=1)\n",
    "df_chunk.dropna(subset=[\"ra\", \"dec\", \"parallax\", \"pmra\", \"pmdec\"], inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.571911Z",
     "start_time": "2025-01-21T17:12:33.548441Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### constrain the Distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_chunk[\"D\"] = 1000/df_chunk[\"parallax\"]\n",
    "\n",
    "df_small = df_chunk[(df_chunk.D > 10) & (df_chunk.D < 1000)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.576008Z",
     "start_time": "2025-01-21T17:12:33.573807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# set variables manually as there is no coarse segmentation in the input dataset right now\n",
    "chunk = 0\n",
    "result_path = output_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.578421Z",
     "start_time": "2025-01-21T17:12:33.576507Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "          ra       dec  ra_error  dec_error  parallax  parallax_error   pmra  \\\n0  85.551753 -2.086052    0.0237     0.0214    2.5119          0.0274  1.424   \n1  85.386485 -1.965999    0.0220     0.0207    2.6756          0.0285  2.093   \n3  85.260554 -1.727195    1.1237     1.0010    3.6669          1.4756 -1.335   \n5  85.321513 -1.731072    0.2782     0.2881    2.4825          0.4087  0.377   \n7  85.326645 -1.831449    0.1863     0.1996    2.7434          0.3040 -1.510   \n\n   pmra_error  pmdec  pmde_error           X           Y           Z  \\\n0       0.028  0.330       0.024 -341.124422 -172.016934 -111.946015   \n1       0.026 -1.232       0.024 -320.641056 -160.350540 -105.662402   \n3       1.221  0.369       1.042 -234.531613 -115.851212  -77.098112   \n5       0.322 -0.389       0.322 -346.419964 -171.370401 -113.530330   \n7       0.257 -0.343       0.231 -313.149436 -155.558950 -102.991515   \n\n    v_a_lsr   v_d_lsr           D  \n0  2.251009  7.092503  398.105020  \n1  3.224025  4.322255  373.747944  \n3 -2.246544  7.052169  272.709918  \n5  0.216861  5.831167  402.819738  \n7 -3.110767  5.951858  364.511190  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ra</th>\n      <th>dec</th>\n      <th>ra_error</th>\n      <th>dec_error</th>\n      <th>parallax</th>\n      <th>parallax_error</th>\n      <th>pmra</th>\n      <th>pmra_error</th>\n      <th>pmdec</th>\n      <th>pmde_error</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Z</th>\n      <th>v_a_lsr</th>\n      <th>v_d_lsr</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>85.551753</td>\n      <td>-2.086052</td>\n      <td>0.0237</td>\n      <td>0.0214</td>\n      <td>2.5119</td>\n      <td>0.0274</td>\n      <td>1.424</td>\n      <td>0.028</td>\n      <td>0.330</td>\n      <td>0.024</td>\n      <td>-341.124422</td>\n      <td>-172.016934</td>\n      <td>-111.946015</td>\n      <td>2.251009</td>\n      <td>7.092503</td>\n      <td>398.105020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85.386485</td>\n      <td>-1.965999</td>\n      <td>0.0220</td>\n      <td>0.0207</td>\n      <td>2.6756</td>\n      <td>0.0285</td>\n      <td>2.093</td>\n      <td>0.026</td>\n      <td>-1.232</td>\n      <td>0.024</td>\n      <td>-320.641056</td>\n      <td>-160.350540</td>\n      <td>-105.662402</td>\n      <td>3.224025</td>\n      <td>4.322255</td>\n      <td>373.747944</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>85.260554</td>\n      <td>-1.727195</td>\n      <td>1.1237</td>\n      <td>1.0010</td>\n      <td>3.6669</td>\n      <td>1.4756</td>\n      <td>-1.335</td>\n      <td>1.221</td>\n      <td>0.369</td>\n      <td>1.042</td>\n      <td>-234.531613</td>\n      <td>-115.851212</td>\n      <td>-77.098112</td>\n      <td>-2.246544</td>\n      <td>7.052169</td>\n      <td>272.709918</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>85.321513</td>\n      <td>-1.731072</td>\n      <td>0.2782</td>\n      <td>0.2881</td>\n      <td>2.4825</td>\n      <td>0.4087</td>\n      <td>0.377</td>\n      <td>0.322</td>\n      <td>-0.389</td>\n      <td>0.322</td>\n      <td>-346.419964</td>\n      <td>-171.370401</td>\n      <td>-113.530330</td>\n      <td>0.216861</td>\n      <td>5.831167</td>\n      <td>402.819738</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>85.326645</td>\n      <td>-1.831449</td>\n      <td>0.1863</td>\n      <td>0.1996</td>\n      <td>2.7434</td>\n      <td>0.3040</td>\n      <td>-1.510</td>\n      <td>0.257</td>\n      <td>-0.343</td>\n      <td>0.231</td>\n      <td>-313.149436</td>\n      <td>-155.558950</td>\n      <td>-102.991515</td>\n      <td>-3.110767</td>\n      <td>5.951858</td>\n      <td>364.511190</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.589186Z",
     "start_time": "2025-01-21T17:12:33.580581Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering\n",
    "\n",
    "Currently written for the scenario that the input is a single chunk without need for further splitting.\n",
    "\n",
    "### A) Preliminary solution -- Gaia data\n",
    "\n",
    "My idea is to make the preliminary cluster using the subset of the sources with Gaia data -- this way we should be able to determine the scaling factors nicely. *Maybe I will need to add more Gaia sources in the region if it does not work at first.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def setup_4D(df_fit: pd.DataFrame, sf_range: Union[list, np.linspace, range],\n",
    "                  KNN_list: Union[list, np.linspace, range], beta: float, knn_initcluster_graph: int,\n",
    "                  scaling: str = None, means=None):\n",
    "\n",
    "    # resampling is not implemented for ICRS yet\n",
    "    n_resampling = 0\n",
    "\n",
    "    # scale the ICRS parameters\n",
    "    scaled_cols = ['ra', 'dec', 'pmra', 'pmdec']\n",
    "    df_fin = df_fit\n",
    "    scale_factors = means\n",
    "\n",
    "    # SigMA kwargs\n",
    "    sigma_kwargs = dict(cluster_features=scaled_cols, scale_factors=scale_factors, nb_resampling=n_resampling,\n",
    "                        max_knn_density=max(KNN_list) + 1, beta=beta, knn_initcluster_graph=knn_initcluster_graph)\n",
    "\n",
    "    setup_dict = {\"scale_factor_list\": sf_range, \"scale_factors\": scale_factors, \"sigma_kwargs\": sigma_kwargs}\n",
    "\n",
    "    return setup_dict, df_fin"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.608152Z",
     "start_time": "2025-01-21T17:12:33.591069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def setup_ICRS_ps(df_fit: pd.DataFrame, sf_params: Union[str, list], sf_range: Union[list, np.linspace, range],\n",
    "                  KNN_list: Union[list, np.linspace, range], beta: float, knn_initcluster_graph: int,\n",
    "                  scaling: str = None, means=None, cols_to_scale = None):\n",
    "\n",
    "    n_resampling = 0\n",
    "\n",
    "    # scale the ICRS parameters\n",
    "    if type(scaling) == str:\n",
    "        df_scaled = df_fit.copy()\n",
    "        if cols_to_scale is None:\n",
    "            cols_to_scale = ['ra', 'dec', 'parallax', 'pmra', 'pmdec']\n",
    "        else:\n",
    "            cols_to_scale = cols_to_scale\n",
    "        scaled_cols = ['ra_scaled', 'dec_scaled', 'parallax_scaled', 'pmra_scaled', 'pmdec_scaled']\n",
    "        scaled_data = [parameter_scaler(df_scaled[col], scaling) for col in cols_to_scale]\n",
    "\n",
    "        for col_id, col in enumerate(scaled_cols):\n",
    "            df_scaled[col] = scaled_data[col_id]\n",
    "\n",
    "        df_fin = df_scaled\n",
    "\n",
    "    else:\n",
    "        scaled_cols = cols_to_scale\n",
    "        df_fin = df_fit\n",
    "\n",
    "    if type(sf_params) == str:\n",
    "        mean_sf, scale_factors = single_parameter_scaling(sf_params, sf_range)\n",
    "    elif type(sf_params) == list:\n",
    "        scale_factors = means\n",
    "\n",
    "    # SigMA kwargs\n",
    "    sigma_kwargs = dict(cluster_features=scaled_cols, scale_factors=scale_factors, nb_resampling=n_resampling,\n",
    "                        max_knn_density=max(KNN_list) + 1, beta=beta, knn_initcluster_graph=knn_initcluster_graph)\n",
    "\n",
    "    setup_dict = {\"scale_factor_list\": sf_range, \"scale_factors\": scale_factors, \"sigma_kwargs\": sigma_kwargs}\n",
    "\n",
    "    return setup_dict, df_fin\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.608417Z",
     "start_time": "2025-01-21T17:12:33.596127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def run_clustering(region_label, df_input, sf_params, parameter_dict: dict, mode: str, output_loc: str, setup_func,\n",
    "                   column_means=None, cols_to_scale =None ):\n",
    "    # most important variables\n",
    "    KNNs = parameter_dict[\"KNN_list\"]\n",
    "    # setup kwargs\n",
    "    if setup_func == \"ICRS\":\n",
    "        setup_kwargs, df_focus = setup_ICRS_ps(df_fit=df_input, sf_params=sf_params, sf_range=parameter_dict[\"sfs\"],\n",
    "                                               KNN_list=KNNs, beta=parameter_dict[\"beta\"],\n",
    "                                               knn_initcluster_graph=parameter_dict[\"knn_initcluster_graph\"],\n",
    "                                               scaling=parameter_dict[\"scaling\"], means=column_means, cols_to_scale=cols_to_scale)\n",
    "    else:\n",
    "        setup_kwargs, df_focus = setup_4D(df_fit = df_input, sf_range=parameter_dict[\"sfs\"],  KNN_list=KNNs, beta=parameter_dict[\"beta\"],\n",
    "                                               knn_initcluster_graph=parameter_dict[\"knn_initcluster_graph\"],\n",
    "                                               scaling=parameter_dict[\"scaling\"], means=column_means, )\n",
    "    sigma_kwargs = setup_kwargs[\"sigma_kwargs\"]\n",
    "    scale_factor_list = setup_kwargs[\"scale_factor_list\"]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # initialize SigMA with sf_mean\n",
    "    clusterer = SigMA(data=df_focus, **sigma_kwargs)\n",
    "    # save X_mean\n",
    "    X_mean_sf = clusterer.X\n",
    "    # initialize array for density values (collect the rho_sums)\n",
    "    rhosum_list = []\n",
    "\n",
    "    # Initialize array for the outer cc (occ) results (remove field stars / rfs, remove spurious clusters / rsc)\n",
    "    results_rfs = np.empty(shape=(len(KNNs), len(df_focus)))\n",
    "    results_rsc = np.empty(shape=(len(KNNs), len(df_focus)))\n",
    "    results_simple = np.empty(shape=(len(KNNs), len(df_focus)))\n",
    "\n",
    "    # Outer loop: KNN\n",
    "    for kid, knn in enumerate(KNNs):\n",
    "\n",
    "        print(f\"-- Current run with KNN = {knn} -- \\n\")\n",
    "\n",
    "        label_matrix_rfs = np.empty(shape=(len(scale_factor_list), len(df_focus)))\n",
    "        label_matrix_rsc = np.empty(shape=(len(scale_factor_list), len(df_focus)))\n",
    "        label_matrix_simple = np.empty(shape=(len(scale_factor_list), len(df_focus)))\n",
    "\n",
    "        # initialize density-sum over all scaling factors\n",
    "        rho_sum = np.zeros(df_focus.shape[0], dtype=np.float32)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        df_labels = pd.DataFrame()\n",
    "        # Inner loop: Scale factors\n",
    "        for sf_id, sf in enumerate(scale_factor_list):\n",
    "            # Set current scale factor\n",
    "            if mode == \"prelim\":\n",
    "                scale_factors = {'pos': {'features': ['parallax_scaled'], 'factor': sf}}\n",
    "                clusterer.set_scaling_factors(scale_factors)\n",
    "                print(f\"Performing clustering for scale factor {clusterer.scale_factors['pos']['factor']}...\")\n",
    "            elif mode == \"final\":\n",
    "                scale_factors = {'pos': {'features': ['ra', 'dec'], 'factor': list(sf[:3])},\n",
    "                                 'vel': {'features': ['pmra', 'pmdec'], 'factor': list(sf[3:])}}\n",
    "                clusterer.set_scaling_factors(scale_factors)\n",
    "                print(f\"Performing clustering for scale factor p{clusterer.scale_factors['pos']['factor']}\"\n",
    "                      f\"{clusterer.scale_factors['vel']['factor']}...\")\n",
    "\n",
    "            # Fit\n",
    "            clusterer.fit(alpha=parameter_dict[\"alpha\"], knn=knn, bh_correction=parameter_dict[\"bh_correction\"])\n",
    "            label_array = clusterer.labels_\n",
    "\n",
    "            # density and X\n",
    "            rho, X = clusterer.weights_, clusterer.X\n",
    "            rho_sum += rho\n",
    "\n",
    "            # a) remove field stars\n",
    "            nb_rfs = remove_field_stars(label_array, rho, label_matrix_rfs, sf_id)\n",
    "            # b) remove spurious clusters\n",
    "            nb_es, nb_rsc = extract_signal_remove_spurious(df_focus, label_array, rho, X, label_matrix_rsc, sf_id)\n",
    "            # c) do new method\n",
    "            nb_simple = extract_signal(label_array, clusterer, label_matrix_simple, sf_id)\n",
    "            # Write the output to the hyperparameter file:\n",
    "\n",
    "            save_output_summary(\n",
    "                summary_str={\"knn\": knn, \"sf\": str(sf), \"n_rfs\": nb_rfs, \"n_rsc\": nb_rsc, \"n_simple\": nb_simple},\n",
    "                file=output_loc + f\"{mode}_ICC_{knn}_summary.csv\")\n",
    "\n",
    "            df_labels[f\"#_{sf_id}\"] = label_matrix_rsc[sf_id, :]\n",
    "\n",
    "        if mode == \"final\":\n",
    "            # save output files\n",
    "            df_labels.to_csv(output_loc + f\"{mode}_ICC_{knn}_labels.csv\")\n",
    "        # append the density sum to the list over all KNN\n",
    "        rhosum_list.append(rho_sum)\n",
    "\n",
    "        # Perform consensus clustering on the a) and b) arrays (automatically generates and saves a html-plot)\n",
    "        labels_icc_rfs, n_icc_rfs = consensus_function(label_matrix_rfs, rho_sum, df_focus,\n",
    "                                                       f\"{mode}_Region_{int(region_label)}_rfs_KNN_{knn}_ICC\",\n",
    "                                                       output_loc,\n",
    "                                                       plotting=True)\n",
    "        labels_icc_rsc, n_icc_rsc = consensus_function(label_matrix_rsc, rho_sum, df_focus,\n",
    "                                                       f\"{mode}_Region_{int(region_label)}_rsc_KNN_{knn}_ICC\",\n",
    "                                                       output_loc,\n",
    "                                                       plotting=True)\n",
    "\n",
    "        labels_icc_simple, n_icc_simple = consensus_function(label_matrix_simple, rho_sum, df_focus,\n",
    "                                                             f\"{mode}_Region_{int(region_label)}_simple_KNN_{knn}_ICC\",\n",
    "                                                             output_loc,\n",
    "                                                             plotting=True)\n",
    "        results_rfs[kid, :] = labels_icc_rfs\n",
    "\n",
    "        results_rsc[kid, :] = labels_icc_rsc\n",
    "        results_simple[kid, :] = labels_icc_simple\n",
    "\n",
    "        print(f\":: Finished run for KNN={knn}! \\n. Found {n_icc_rfs} / {n_icc_rsc} / {n_icc_simple} final clusters.\")\n",
    "\n",
    "    knn_mid = int(len(KNNs) / 2 - 1)\n",
    "    df_save = df_focus.copy()\n",
    "    label_lists = [results_rfs, results_rsc, results_simple]\n",
    "\n",
    "    # Perform consensus clustering on the c) and d) steps\n",
    "    labels_occ, n_occ = zip(\n",
    "        *(consensus_function(jl, rhosum_list[knn_mid], df_focus, f\"{mode}_Region_{int(region_label)}_{name}_OCC\",\n",
    "                             output_loc) for jl, name in zip(label_lists, [\"rfs\", \"rsc\", \"simple\"])))\n",
    "    n_occ = list(n_occ)\n",
    "    labels_occ = list(labels_occ)\n",
    "\n",
    "    # save the labels in a csv file and plot the result\n",
    "    df_save[\"rsc\"] = labels_occ[1]\n",
    "    df_save[\"rfs\"] = labels_occ[0]\n",
    "    df_save[\"simple\"] = labels_occ[2]\n",
    "    df_save.to_csv(output_loc + f\"{mode}_Region_{int(region_label)}_results_CC.csv\")\n",
    "\n",
    "    save_output_summary(summary_str={\"knn\": \"occ\", \"n_rfs\": n_occ[0], \"n_rsc\": n_occ[1],\n",
    "                                     \"n_rfs_cleaned\": n_occ[2]},\n",
    "                        file=output_loc + f\"{mode}_Region_{int(region_label)}_outer_summary.csv\")\n",
    "\n",
    "    # Output log-file\n",
    "    filename = output_loc + f\"Region_{region_label}_{mode}_parameters.txt\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for key, value in parameter_dict.items():\n",
    "            file.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    return df_save\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:12:33.650927Z",
     "start_time": "2025-01-21T17:12:33.598284Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Cluster parameters\n",
    "dict_prelim = dict(alpha=0.01,\n",
    "                   beta=0.99,\n",
    "                   knn_initcluster_graph=35, # slightly larger than the bigget KNN\n",
    "                   KNN_list=[15,20, 25],\n",
    "                   sfs=[0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55],\n",
    "                   scaling=\"robust\",\n",
    "                   bh_correction=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:13:02.960732Z",
     "start_time": "2025-01-21T17:13:02.942827Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART A) Starting clustering ... \n",
      "\n",
      "-- Current run with KNN = 15 -- \n",
      "\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.1...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.15...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.2...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.25...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.3...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.35...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.4...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.45...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.5...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.55...\n",
      "Performing gradient ascend using a 15-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      ":: Finished run for KNN=15! \n",
      ". Found 1 / 1 / 1 final clusters.\n",
      "-- Current run with KNN = 20 -- \n",
      "\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.1...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.15...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.2...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.25...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.3...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.35...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.4...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.45...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.5...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.55...\n",
      "Performing gradient ascend using a 20-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      ":: Finished run for KNN=20! \n",
      ". Found 1 / 1 / 1 final clusters.\n",
      "-- Current run with KNN = 25 -- \n",
      "\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.1...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.15...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.2...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.25...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.3...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.35...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.4...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.45...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.5...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      "Creating k-d trees of resampled data sets...\n",
      "Performing clustering for scale factor 0.55...\n",
      "Performing gradient ascend using a 25-NN density estimation.\n",
      "Updated significance threshold: 5.00e-02\n",
      ":: Finished run for KNN=25! \n",
      ". Found 1 / 1 / 1 final clusters.\n"
     ]
    }
   ],
   "source": [
    "print(f\"PART A) Starting clustering ... \\n\")\n",
    "\n",
    "df_prelim = run_clustering(region_label=chunk, df_input=df_small, sf_params=\"parallax_scaled\",\n",
    "                           parameter_dict=dict_prelim, mode=\"prelim\", output_loc=result_path, setup_func=\"ICRS\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:13:13.401850Z",
     "start_time": "2025-01-21T17:13:03.471563Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n",
      "507\n"
     ]
    },
    {
     "data": {
      "text/plain": "381.38825324180016"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d =df_prelim.dropna()\n",
    "d = d[d.parallax >= 0]\n",
    "\n",
    "print(len(df_prelim))\n",
    "print(len(d))\n",
    "\n",
    "d.rename(columns={\"Plx\":'parallax'}, inplace=True)\n",
    "d[\"radial_velocity\"] = pd.NA\n",
    "d[\"D\"] = 1000/d[\"parallax\"]\n",
    "\n",
    "c1 = d[d[\"rsc\"] == 0]\n",
    "c1.D.median()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:13:16.698943Z",
     "start_time": "2025-01-21T17:13:16.693656Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B) Simulate clusters and determine scale factors\n",
    "\n",
    "We now want to use the preliminary solution (mostly cluster cores) to determine more appropriate scaling factors for the clustering algorithm. We calculate them and save them in a text file. It is also possible to add a couple of small, artificial clusters to make the algorithm more sensitive for small clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART B) Simulating clusters ... \n",
      "\n",
      "380.7199228135706\n",
      "380.7199228135706\n",
      "380.7199228135706\n",
      "404.58926864122793\n",
      "404.58926864122793\n",
      "404.58926864122793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Sigma_Orion/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"PART B) Simulating clusters ... \\n\")\n",
    "\n",
    "stds = calculate_std_devs(input_df=d, SigMA_dict=dict_prelim, sampling_data= error_sampling_df, n_artificial = 0, sample_radius= 2, output_path = result_path, plot_figs = False)\n",
    "\n",
    "# save scaling factors in Data directory\n",
    "directory = \"../../Data/Scale_factors\"\n",
    "filename = f\"sfs_FN_{chunk}.txt\"\n",
    "\n",
    "# Open the file in write mode ('w')\n",
    "with open(f\"{directory}/{filename}\", 'w') as file:\n",
    "    for i, label in enumerate([\"ra\", \"dec\", \"parallax\", \"pmra\", \"pmdec\"]):\n",
    "        print(f\"{label}:\", np.min(stds[i, :]), np.mean(stds[i, :]), np.max(stds[i, :]), file=file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:13:24.892433Z",
     "start_time": "2025-01-21T17:13:24.779439Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### C) Cluster with new SF\n",
    "\n",
    "We apply the new scaling factors to the clustering algorithm. You can play with the number of scaling factors - more factors = more computing time, but hopefully also a more reliable result, as the parameter space is sampled more densely."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# determine the number of SF to draw using lhc_lloyd sampling\n",
    "num_sf = 30"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:13:39.720071Z",
     "start_time": "2025-01-21T17:13:39.699486Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bounds are not consistent 'l_bounds' < 'u_bounds'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# draw number of scale factors\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m sfs, means \u001B[38;5;241m=\u001B[39m \u001B[43mlhc_lloyd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../../Data/Scale_factors/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msfs_FN_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mchunk\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_sf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# determine means for clusterer initialization\u001B[39;00m\n\u001B[1;32m      5\u001B[0m scale_factor_means \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mra\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdec\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparallax\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfactor\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mlist\u001B[39m(means[:\u001B[38;5;241m3\u001B[39m])},\n\u001B[1;32m      6\u001B[0m                       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvel\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpmra\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpmdec\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfactor\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mlist\u001B[39m(means[\u001B[38;5;241m3\u001B[39m:])}}\n",
      "File \u001B[0;32m~/PycharmProjects/Distant_SigMA/DistantSigMA/DistantSigMA/scalefactor_sampling.py:57\u001B[0m, in \u001B[0;36mlhc_lloyd\u001B[0;34m(sfs_file, size)\u001B[0m\n\u001B[1;32m     54\u001B[0m sampler \u001B[38;5;241m=\u001B[39m qmc\u001B[38;5;241m.\u001B[39mLatinHypercube(d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(l_bound), optimization\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlloyd\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     55\u001B[0m samples \u001B[38;5;241m=\u001B[39m sampler\u001B[38;5;241m.\u001B[39mrandom(n\u001B[38;5;241m=\u001B[39msize)\n\u001B[0;32m---> 57\u001B[0m scaled_samples \u001B[38;5;241m=\u001B[39m \u001B[43mqmc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml_bound\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh_bound\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m column_means \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(scaled_samples, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m scaled_samples, column_means\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Sigma_Orion/lib/python3.9/site-packages/scipy/stats/_qmc.py:156\u001B[0m, in \u001B[0;36mscale\u001B[0;34m(sample, l_bounds, u_bounds, reverse)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sample\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSample is not a 2D array\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 156\u001B[0m lower, upper \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_bounds\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43ml_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ml_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mu_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mu_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m reverse:\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# Checking that sample is within the hypercube\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (sample\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (sample\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.\u001B[39m):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Sigma_Orion/lib/python3.9/site-packages/scipy/stats/_qmc.py:2568\u001B[0m, in \u001B[0;36m_validate_bounds\u001B[0;34m(l_bounds, u_bounds, d)\u001B[0m\n\u001B[1;32m   2565\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m   2567\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39mall(lower \u001B[38;5;241m<\u001B[39m upper):\n\u001B[0;32m-> 2568\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBounds are not consistent \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml_bounds\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m < \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu_bounds\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2570\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m lower, upper\n",
      "\u001B[0;31mValueError\u001B[0m: Bounds are not consistent 'l_bounds' < 'u_bounds'"
     ]
    }
   ],
   "source": [
    "# draw number of scale factors\n",
    "sfs, means = lhc_lloyd('../../Data/Scale_factors/' + f\"sfs_FN_{chunk}.txt\", num_sf)\n",
    "\n",
    "# determine means for clusterer initialization\n",
    "scale_factor_means = {'pos': {'features': ['ra', 'dec', 'parallax'], 'factor': list(means[:3])},\n",
    "                      'vel': {'features': ['pmra', 'pmdec'], 'factor': list(means[3:])}}\n",
    "\n",
    "# dict for final clustering\n",
    "dict_final = dict(alpha=0.01,\n",
    "                  beta=0.99,\n",
    "                  knn_initcluster_graph=35,\n",
    "                  KNN_list=[20, 25, 30],\n",
    "                  sfs=sfs,\n",
    "                  scaling=None,\n",
    "                  bh_correction=False)\n",
    "\n",
    "\n",
    "df_fin = run_clustering(region_label=chunk, df_input=df_chunk,\n",
    "                                sf_params=[\"ra\", \"dec\", \"parallax\", \"pmra\", \"pmdec\"],\n",
    "                                parameter_dict=dict_final, mode=\"final\", output_loc=result_path,\n",
    "                                column_means=scale_factor_means)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T17:13:40.296113Z",
     "start_time": "2025-01-21T17:13:40.091840Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output\n",
    "\n",
    "The code produces a lot of output plots (html). They can be split into the overall results (OCC), which show three different noise removal methods:\n",
    "- rfs (less strict, sometimes spurious clusters)\n",
    "- rsc (more strict, default solution for me)\n",
    "- simple (between rfs and rsc)\n",
    "\n",
    "It also produces plots for the individual solutions for the different KNN."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
